{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captioning Model Builder\n",
    "\n",
    "### [과정]\n",
    "### **1) 캡션 텍스트 전처리 및 임베딩 준비**<br>\n",
    "텍스트를 단어별로 나누는 토큰화 작업을 하고, 단어들을 숫자로 변환한 후, 이를 임베딩 벡터로 변환할 준비 해두기<br>\n",
    "-> 추후 LSTM에서 사용 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Mon Oct 14 21:03:24 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce MX450         WDDM  |   00000000:2D:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8             N/A / ERR!  |       0MiB /   2048MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     12120    C+G   ...5edd4fcd19e0\\EasyConnectManager.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU 체크\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(gpu_devices) > 0:\n",
    "    print(f\"Connected to GPU: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"Not connected to a GPU\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your system has 8.2 gigabytes of available RAM\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RAM 체크\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print('Your system has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
    "print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 버전 확인\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Hub MSCOCO 한국어 이미지 설명 데이터셋\n",
    "\n",
    "[기본 정보]<br>\n",
    "- 전체 : 123,287\n",
    "- train data : 82,783\n",
    "- validation data : 40,504\n",
    "\n",
    "**[문제] 데이터셋에 이미지가 포함이 안 되어 있음**<br>\n",
    "-> 어떻게 처리하지?<br>\n",
    "일단 데이터 보니까 안에 이미지의 url 주소가 있음 <- 이거로 다운받아 와야 할듯 ,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "data_path = os.path.join(\"data\", \"MSCOCO_train_val_Korean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일 열기\n",
    "with open(data_path, 'r', encoding='UTF8') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in the dataset: 123287\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(f\"Total entries in the dataset: {len(json_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'val2014/COCO_val2014_000000391895.jpg',\n",
       " 'captions': ['A man with a red helmet on a small moped on a dirt road. ',\n",
       "  'Man riding a motor bike on a dirt road on the countryside.',\n",
       "  'A man riding on the back of a motorcycle.',\n",
       "  'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "  'A man in a red shirt and a red hat is on a motorcycle on a hill side.'],\n",
       " 'id': 391895,\n",
       " 'caption_ko': ['빨간 헬멧을 쓴 남자가 작은 모터 달린 비포장 도로를 달려 있다.',\n",
       "  '시골의 비포장 도로에서 오토바이를 타는 남자',\n",
       "  '오토바이 뒤에 탄 남자',\n",
       "  '오토바이 위에 젊은이가 탄 비포장 도로는 다리가 있는 초록빛 지역의 전경과 구름 낀 산의 배경이 있다.',\n",
       "  '빨간 셔츠와 빨간 모자를 쓴 남자가 언덕 쪽 오토바이 위에 있다.']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터 확인\n",
    "json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path of the first entry: val2014/COCO_val2014_000000391895.jpg\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 데이터의 파일 경로 확인\n",
    "print(f\"File path of the first entry: {json_data[0]['file_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 82783\n",
      "Number of val samples: 40504\n"
     ]
    }
   ],
   "source": [
    "# 'train2014'와 'val2014'로 나뉜 데이터의 개수 세기\n",
    "train_data = [item for item in json_data if item['file_path'].startswith('train2014')]\n",
    "val_data = [item for item in json_data if item['file_path'].startswith('val2014')]\n",
    "\n",
    "print(f\"Number of train samples: {len(train_data)}\")\n",
    "print(f\"Number of val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSCOCO 이미지 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 저장을 위한 디렉토리 생성 (이미 존재하면 무시)\n",
    "current_dir = os.getcwd()\n",
    "images_dir = os.path.join(current_dir, 'images')  # 변경할 경로 설정\n",
    "\n",
    "train_zip_path = os.path.join(images_dir, 'train2014.zip')\n",
    "val_zip_path = os.path.join(images_dir, 'val2014.zip')\n",
    "\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\김소연\\Desktop\\soyeon\\ShortClip-Caption-Generator\\images\n"
     ]
    }
   ],
   "source": [
    "# 경로 이동\n",
    "os.chdir(images_dir)\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 다운로드 함수\n",
    "def download_file(url, destination):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(destination, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train2014.zip already exists, skipping download.\n",
      "val2014.zip already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# train2014.zip 다운로드 여부 확인\n",
    "if not os.path.exists(train_zip_path):\n",
    "    print(\"Downloading train2014.zip...\")\n",
    "    download_file('http://images.cocodataset.org/zips/train2014.zip', train_zip_path)\n",
    "else:\n",
    "    print(\"train2014.zip already exists, skipping download.\")\n",
    "\n",
    "# val2014.zip 다운로드 여부 확인\n",
    "if not os.path.exists(val_zip_path):\n",
    "    print(\"Downloading val2014.zip...\")\n",
    "    download_file('http://images.cocodataset.org/zips/val2014.zip', val_zip_path)\n",
    "else:\n",
    "    print(\"val2014.zip already exists, skipping download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 해제 함수\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train2014 already extracted, skipping extraction.\n",
      "val2014 already extracted, skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "# train2014 압축 해제 여부 확인\n",
    "train_images_dir = os.path.join(images_dir, 'train2014')\n",
    "if not os.path.exists(train_images_dir):\n",
    "    print(\"Extracting train2014.zip...\")\n",
    "    extract_zip(train_zip_path, images_dir)\n",
    "else:\n",
    "    print(\"train2014 already extracted, skipping extraction.\")\n",
    "\n",
    "# val2014 압축 해제 여부 확인\n",
    "val_images_dir = os.path.join(images_dir, 'val2014')\n",
    "if not os.path.exists(val_images_dir):\n",
    "    print(\"Extracting val2014.zip...\")\n",
    "    extract_zip(val_zip_path, images_dir)\n",
    "else:\n",
    "    print(\"val2014 already extracted, skipping extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train2014: 82783\n",
      "Number of images in val2014: 40504\n",
      "Total dataset size: 123287\n"
     ]
    }
   ],
   "source": [
    "# 이미지 개수 확인\n",
    "train_images_path = os.path.join(images_dir, 'train2014')\n",
    "val_images_path = os.path.join(images_dir, 'val2014')\n",
    "\n",
    "train_image_count = len(os.listdir(train_images_path))\n",
    "val_image_count = len(os.listdir(val_images_path))\n",
    "\n",
    "print(f\"Number of images in train2014: {train_image_count}\")\n",
    "print(f\"Number of images in val2014: {val_image_count}\")\n",
    "\n",
    "# 데이터셋 개수 확인\n",
    "print(f\"Total dataset size: {train_image_count + val_image_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\김소연\\Desktop\\soyeon\\ShortClip-Caption-Generator\n"
     ]
    }
   ],
   "source": [
    "# 불용어 파일들이 위치한 디렉토리 경로\n",
    "os.chdir('..')\n",
    "current_dir = os.getcwd()\n",
    "input_dir = os.path.join(current_dir, 'stopwords')\n",
    "print(current_dir)\n",
    "output_file = 'combined_stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_stopwords.txt does not exist. Combining stopwords...\n",
      "Combined stopwords saved to combined_stopwords.txt.\n"
     ]
    }
   ],
   "source": [
    "# 불용어 파일이 이미 존재하는지 확인\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"{output_file} already exists. Skipping combining stopwords.\")\n",
    "else:\n",
    "    print(f\"{output_file} does not exist. Combining stopwords...\")\n",
    "    \n",
    "    # 중복을 제거한 불용어들을 저장할 세트\n",
    "    final_stopwords = set()\n",
    "\n",
    "    # stopwords 폴더 내 모든 txt 파일을 읽어 중복 제거\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.txt'):  # .txt 파일만 처리\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                stopwords = f.readlines()\n",
    "                # 각 파일에서 불용어를 추가하고 중복을 자동으로 제거\n",
    "                final_stopwords.update(word.strip() for word in stopwords)\n",
    "\n",
    "    # 중복이 제거된 불용어 리스트를 combined_stopwords.txt 파일에 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for word in sorted(final_stopwords):  # 정렬하여 저장 (선택 사항)\n",
    "            f.write(word + '\\n')\n",
    "\n",
    "    print(f\"Combined stopwords saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캡션 텍스트 전처리 및 토큰화\n",
    "\n",
    "불용어들을 제거하고 의미가 있는 형태소만 남기기<br>\n",
    "-> 의미가 있는 핵심 단어들만 담아서 정제된 상태로 핵심 정보만 남겨서 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COCO_val2014_000000391895.jpg'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]['file_path'].split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 및 변수 초기화\n",
    "okt = Okt()\n",
    "image_caption_dict = dict()\n",
    "sent_token = []\n",
    "max_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 파일 경로\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "stopwords_file = os.path.join(current_dir, 'combined_stopwords.txt')\n",
    "output_file = os.path.join(current_dir, 'image_caption_dict.pkl')  # 처리된 결과 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_stopwords.txt 파일에서 불용어 불러오기\n",
    "with open(stopwords_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    stopwords = set(line.strip() for line in f.readlines())  # 불용어를 set으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\김소연\\Desktop\\soyeon\\ShortClip-Caption-Generator\\image_caption_dict.pkl already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "# 결과 파일이 이미 존재하는지 확인\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"{output_file} already exists. Skipping processing.\")\n",
    "\n",
    "    # 파일이 존재하면 데이터 로드\n",
    "    with open(output_file, \"rb\") as f:\n",
    "        image_caption_dict = pickle.load(f)\n",
    "\n",
    "    # 로드된 데이터를 바탕으로 max_length 다시 계산\n",
    "    for desc_list in image_caption_dict.values():\n",
    "        for desc in desc_list:\n",
    "            desc_words = desc.split()\n",
    "            max_length = max(max_length, len(desc_words))\n",
    "\n",
    "else:\n",
    "    # 파일이 존재하지 않으면 데이터 처리\n",
    "    print(f\"{output_file} does not exist. Processing data...\")\n",
    "\n",
    "    # 이미지 및 캡션 처리\n",
    "    for entry in tqdm(json_data):\n",
    "        id = entry['file_path'].split('/')[1]  # jpg 파일 이름 추출\n",
    "        descs = []\n",
    "\n",
    "        for desc in entry['caption_ko']:\n",
    "            # 전처리 및 형태소 분석\n",
    "            desc = re.sub('[^가-힣 ]', '', desc)  # 한글 외 제거\n",
    "            desc_words = [word for word in okt.morphs(desc, stem=True) if word not in stopwords]  # 형태소 분석 및 불용어 제거\n",
    "            \n",
    "            # 토큰 리스트 및 최대 길이 갱신\n",
    "            sent_token.append(desc_words)\n",
    "            max_length = max(max_length, len(desc_words))\n",
    "\n",
    "            # 형태소 리스트를 하나의 문자열로 변환 후 저장\n",
    "            descs.append(' '.join(desc_words))\n",
    "\n",
    "        # 이미지 ID를 key로, 5개의 description 리스트를 value로 가진 dictionary 생성\n",
    "        image_caption_dict[id] = descs\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(image_caption_dict, f)\n",
    "    print(f\"{output_file} has been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 39\n",
      "number of {image-descs}: 123287\n",
      "img_desc_dict looks like: ('COCO_val2014_000000391895.jpg', ['빨갛다 헬멧 쓸다 남자 작다 모터 달리다 비 포장 도로 달다', '시골 비 포장 도로 오토바이 남자', '오토바이 뒤 남자', '오토바이 위 젊은이 비 포장 도로 다리 초록빛 지역 전경 구름 끼다 산 배경', '빨갛다 셔츠 빨갛다 모자 쓸다 남자 언덕 쪽 오토바이 위'])\n",
      "total number of unique words: 15496\n"
     ]
    }
   ],
   "source": [
    "# 전체 고유 단어 추출\n",
    "unique_words = {word for token_list in image_caption_dict.values() for token in token_list for word in token.split()}\n",
    "\n",
    "# 결과 출력\n",
    "print('max length:', max_length)  # 가장 긴 캡션 길이\n",
    "print('number of {image-descs}:', len(image_caption_dict))  # 총 이미지-캡션 데이터 개수\n",
    "print('img_desc_dict looks like:', list(image_caption_dict.items())[0])  # 예시 데이터 확인\n",
    "print('total number of unique words:', len(unique_words))  # 고유 단어 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조사, 어미 등과 같은 문법적 관계를 포현하는 형태소도 추가해주기<br>\n",
    "-> 나중에 자연스러운 문장을 생성할 수 있도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 및 변수 초기화\n",
    "okt = Okt()\n",
    "image_caption_dict = dict()\n",
    "sent_token = []\n",
    "max_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일 경로 설정\n",
    "output_file = os.path.join(os.getcwd(), \"image_caption_dict2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\김소연\\Desktop\\soyeon\\ShortClip-Caption-Generator\\image_caption_dict2.pkl does not exist. Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [31:52<00:00, 64.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\김소연\\Desktop\\soyeon\\ShortClip-Caption-Generator\\image_caption_dict2.pkl has been created and saved.\n"
     ]
    }
   ],
   "source": [
    "# 결과 파일이 이미 존재하는지 확인\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"{output_file} already exists. Loading data...\")\n",
    "\n",
    "    # 파일이 존재하면 데이터 로드\n",
    "    with open(output_file, \"rb\") as f:\n",
    "        image_caption_dict = pickle.load(f)\n",
    "\n",
    "    # 로드된 데이터를 바탕으로 max_length 다시 계산\n",
    "    for desc_list in image_caption_dict.values():\n",
    "        for desc in desc_list:\n",
    "            desc_words = desc.split()\n",
    "            max_length = max(max_length, len(desc_words))\n",
    "\n",
    "else:\n",
    "    print(f\"{output_file} does not exist. Processing data...\")\n",
    "\n",
    "    # json_data에 대한 처리\n",
    "    for entry in tqdm(json_data):\n",
    "        # 이미지 파일 이름 추출\n",
    "        id = entry['file_path'].split('/')[1]  # jpg 파일 이름 추출\n",
    "        descs = []\n",
    "\n",
    "        # 각 이미지에 대한 5개의 캡션 처리\n",
    "        for desc in entry['caption_ko']:\n",
    "            desc = re.sub('[^가-힣 ]', '', desc) # 전처리: 한글만 남기고 나머지 제거\n",
    "            desc_words = okt.morphs(desc) # 형태소 분석 (의미형태소 + 기능형태소)\n",
    "\n",
    "            sent_token.append(desc_words) # 임베딩용 토큰 리스트 추가\n",
    "\n",
    "            # 최대 길이 업데이트\n",
    "            max_length = max(max_length, len(desc_words))\n",
    "\n",
    "            # 형태소 리스트를 공백으로 결합한 후 저장\n",
    "            descs.append(' '.join(desc_words))\n",
    "\n",
    "        # 이미지 이름을 key로, 5개의 전처리된 캡션 리스트를 value로 가진 dictionary 생성\n",
    "        image_caption_dict[id] = descs\n",
    "\n",
    "    # 처리 후 결과를 .pkl 파일로 저장\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(image_caption_dict, f)\n",
    "    print(f\"{output_file} has been created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 55\n",
      "number of image-descriptions: 123287\n",
      "img_desc_dict example: ('COCO_val2014_000000391895.jpg', ['빨간 헬멧 을 쓴 남자 가 작은 모터 달린 비 포장 도로 를 달려 있다', '시골 의 비 포장 도로 에서 오토바이 를 타는 남자', '오토바이 뒤 에 탄 남자', '오토바이 위 에 젊은이 가 탄 비 포장 도로 는 다리 가 있는 초록빛 지역 의 전경 과 구름 낀 산 의 배경 이 있다', '빨간 셔츠 와 빨간 모자 를 쓴 남자 가 언덕 쪽 오토바이 위 에 있다'])\n",
      "total number of unique words: 24719\n"
     ]
    }
   ],
   "source": [
    "# 전체 고유 단어 추출\n",
    "unique_words = set(word for token_list in image_caption_dict.values() for token in token_list for word in token.split())\n",
    "\n",
    "# 결과 출력 (파일이 있든 없든 항상 실행)\n",
    "print(f'max length: {max_length}')  # 가장 긴 캡션의 길이\n",
    "print(f'number of image-descriptions: {len(image_caption_dict)}')  # 총 이미지-캡션 데이터 개수\n",
    "print(f'img_desc_dict example: {list(image_caption_dict.items())[0]}')  # 첫 번째 데이터 예시\n",
    "print(f'total number of unique words: {len(unique_words)}')  # 고유 단어 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"image_caption_dict2.pkl\", \"rb\") as f:\n",
    "    img_desc_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['빨간', '헬멧', '을', '쓴', '남자', '가', '작은', '모터', '달린', '비', '포장', '도로', '를', '달려', '있다']\n",
      "['시골', '의', '비', '포장', '도로', '에서', '오토바이', '를', '타는', '남자']\n",
      "['오토바이', '뒤', '에', '탄', '남자']\n",
      "['오토바이', '위', '에', '젊은이', '가', '탄', '비', '포장', '도로', '는', '다리', '가', '있는', '초록빛', '지역', '의', '전경', '과', '구름', '낀', '산', '의', '배경', '이', '있다']\n",
      "['빨간', '셔츠', '와', '빨간', '모자', '를', '쓴', '남자', '가', '언덕', '쪽', '오토바이', '위', '에', '있다']\n"
     ]
    }
   ],
   "source": [
    "for x in sent_token[:5]:\n",
    "  print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
